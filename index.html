<!DOCTYPE html>

<head>
    <meta charset="utf-8" />
    <!-- model viewer -->
    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>
    <title>LiT: Delving into a Simplified Linear Diffusion Transformer for Image Generation</title>
    <link rel="icon" type="image/x-icon" href="../assets/css/images/favicon.ico">
    <meta content="LiT: Delving into a Simplified Linear Diffusion Transformer for Image Generation" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MLDP9MKGC8"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-MLDP9MKGC8');
    </script>
</head>

<body>
    <!-- title -->
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">LiT: Delving into a Simplified Linear Diffusion Transformer for Image Generation</h1>
            <div class="nerf_subheader_v2">Arxiv 2025.01</div>
            <div class="nerf_subheader_v2">
                <div>
                    <a href="https://scholar.google.com/citations?user=QjVR3UUAAAAJ&hl=zh-CN" target="_blank"
                        class="nerf_authors_v2">Jiahao Wang<span class="text-span_nerf"></span></a>
                    <sup>1†</sup>,&nbsp;&nbsp;
                    <a href="https://openreview.net/profile?id=~Ning_Kang2" target="_blank"
                        class="nerf_authors_v2">Ning Kang<span class="text-span_nerf"></span></a>
                    <sup>3</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=hqDyTg8AAAAJ&hl=en" target="_blank"
                        class="nerf_authors_v2">Lewei Yao<span class="text-span_nerf"></span></a>
                    <sup>3</sup>,&nbsp;&nbsp;
                    <a href="https://chenmnz.github.io/" target="_blank"
                        class="nerf_authors_v2">Mengzhao Chen<span class="text-span_nerf"></span></a>
                    <sup>1</sup>,&nbsp;&nbsp;
                    <a href="https://hills-code.github.io/" target="_blank"
                        class="nerf_authors_v2">Chengyue Wu<span class="text-span_nerf"></span></a>
                    <sup>1</sup>,&nbsp;&nbsp;
                    <a href="https://www.zhangsongyang.com/" target="_blank"
                        class="nerf_authors_v2">Songyang Zhang<span class="text-span_nerf"></span></a>
                    <sup>2</sup>,&nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=aA70TOwAAAAJ&hl=en" target="_blank"
                        class="nerf_authors_v2">Shuchen Xue<span class="text-span_nerf"></span></a>
                    <sup>4</sup>,&nbsp;&nbsp;
                    <a href="https://yongliu20.github.io/" target="_blank"
                        class="nerf_authors_v2">Yong Liu<span class="text-span_nerf"></span></a>
                    <sup>5</sup>,&nbsp;&nbsp;
                    <a href="https://wutaiqiang.github.io/" target="_blank"
                        class="nerf_authors_v2">Taiqiang Wu<span class="text-span_nerf"></span></a>
                    <sup>1</sup>,&nbsp;&nbsp;
                    <a href="https://xh-liu.github.io/" target="_blank"
                        class="nerf_authors_v2">Xihui Liu<span class="text-span_nerf"></span></a>
                    <sup>1</sup>,&nbsp;&nbsp;
                    <a href="https://kpzhang93.github.io/" target="_blank"
                        class="nerf_authors_v2">Kaipeng Zhang<span class="text-span_nerf"></span></a>
                    <sup>2</sup>,&nbsp;&nbsp;
                    <a href="https://openreview.net/profile?id=~Shifeng_Zhang5" target="_blank"
                        class="nerf_authors_v2">Shifeng Zhang<span class="text-span_nerf"></span></a>
                    <sup>3</sup>,&nbsp;&nbsp;
                    <a href="https://wqshao126.github.io/" target="_blank"
                        class="nerf_authors_v2">Wenqi Shao<span class="text-span_nerf"></span></a>
                    <sup>2†</sup>,&nbsp;&nbsp;
                    <a href="https://zhenguol.github.io/" target="_blank"
                        class="nerf_authors_v2">Zhenguo Li<span class="text-span_nerf"></span></a>
                    <sup>3†</sup>,&nbsp;&nbsp;
                    <a href="http://luoping.me/" target="_blank"
                        class="nerf_authors_v2">Ping Luo<span class="text-span_nerf"></span></a>
                    <sup>1</sup>
                </div>
                <div>
                    <h1 class="nerf_affiliation_v2"><sup>1 </sup>HKU</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>2 </sup>Shanghai AI Lab</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>3 </sup>Huawei Noah's Ark Lab</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>4 </sup>UCAS</h1>,&nbsp;&nbsp;
                    <h1 class="nerf_affiliation_v2"><sup>5 </sup>THUsz</h1>,
                </div>
                <div>
                    <!-- <span class="nerf_affiliation_v2">* Equal contribution</span>, -->
                    <span class="nerf_affiliation_v2">† Corresponding author</span>,
                    <!-- <span class="nerf_affiliation_v2">‡ Project leader</span>, -->
                </div>

                <div class="external-link">
                    <a class="btn" href="https://arxiv.org/pdf/2501.12976" role="button" target="_blank">
                        <i class="ai ai-arxiv"></i> Arxiv </a>
                    <!-- <a class="btn" href="paper/2412.14169v1.pdf" role="button" target="_blank">
                        <i class="fa fa-file-pdf"></i> Paper </a> -->
                    <a class="btn" href="https://github.com/techmonsterwang/LiT" role="button" target="_blank" disabled>
                        <i class="fa-brands fa-github"></i> Code </a>
                    <!-- <a class="btn btn-large btn-light" href="https://huggingface.co/spaces/BAAI/nova-d48w1024-sdxl1024"
                        role="button" target="_blank" disabled>
                        <i class="fas fa-robot"></i> T2I Demo </a> -->
                    <a class="btn btn-large btn-light" href="https://www.youtube.com/watch?v=X8aIrYYjFKU&t=7s"
                        role="button" target="_blank" disabled>
                        <i class="fas fa-robot"></i> Laptop Demo </a>
                </div>

            </div>
        </div>

    </div>

    <!-- teasers-->
    <div class="white_section_nerf  w-container">
        <div class="grid-container-1">
            <img src="assets/images/laptop.png">
            <p><strong>LiT</strong> can run on the edge-side <strong>laptop</strong> in an offline manner, generating 1K resolution photorealistic images.
            </p>
        </div>
    </div>

    <!-- abstarct -->
    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf_h2">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
                In commonly used sub-quadratic complexity modules,
                linear attention benefits from simplicity and high parallelism, making it promising for image synthesis tasks. However, the architectural design and learning strategy for linear attention remain underexplored in this field. In this
                paper, we offer a suite of ready-to-use solutions for efficient linear diffusion Transformers. Our core contributions include: <strong>(1) Simplified Linear Attention</strong> using <strong>few
                heads</strong>, observing the free-lunch effect of performance without latency increase. <strong>(2) Weight inheritance from a fully
                pre-trained diffusion Transformer:</strong> initializing linear Transformer using pre-trained diffusion Transformer and loading
                all parameters except for those related to linear attention.
            <strong>(3) Hybrid knowledge distillation objective:</strong> using a pretrained diffusion Transformer to help the training of the student linear Transformer, supervising not only the predicted
                noise but also the variance of the reverse diffusion process.
                These guidelines lead to our proposed <strong>Linear Diffusion
                    Transformer (LiT)</strong>, an efficient text-to-image Transformer
                that can be deployed offline on a laptop. Experiments show
                that in class-conditional 256×256 and 512×512 ImageNet
                benchmark LiT achieves highly competitive FID while reducing training steps by 80% and 77% compared to DiT.
                LiT also rivals methods based on Mamba or Gated Linear
                Attention. Besides, for text-to-image generation, LiT allows
                for the rapid synthesis of up to 1K resolution photorealistic
                images.
                <br>
            </p>
        </div>
    </div>


    <!-- Method Overview -->
    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf_h2">Method</h2>
        <div class="grid-container-1">
            <img src="assets/images/simple.png" style="width: 100%; max-width: 800px;">
            <!-- <img src="assets/images/nova_framework.png"> -->
            <p> Compared to Mamba SSM and gated linear attention, <strong>linear attention</strong> achieves sub-quadratic computational
                complexity with a <strong>remarkably simple design</strong>, and it does not depend on any recurrent states.
            </p>
        </div>
        <div class="grid-container-1">
            <img src="assets/images/guideline.png" style="width: 100%; max-width: 800px;">
            <!-- <img src="assets/images/attention.png"> -->
            <p> <strong>Overall training procedure of LiT</strong>. Following the macro/micro-level design of DiT (for class-conditioned image generation)
                and PixArt-Σ (for text-to-image generation), LiT replace the self-attention in each block with the linear attention. We linearize diffusion
                Transformers by (1) building a strong linear DiT baseline with few heads, (2) inheriting weights from a DiT teacher and (3) distilling useful
                knowledge (predicted noise and the variances of the reverse diffusion process) from the teacher model.
            </p>
        </div>
    </div>

    <!-- Quantitative Results -->
    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf_h2">Class-Conditional Image Generation on ImageNet</h2>
        <div class="grid-container-1">
            <!-- <h3 class="grey-heading_nerf_h3">Text-to-image evaluation on various benchmarks.</h3> -->
            <!-- <p><strong>Text-to-image evaluation on various benchmarks.</strong> -->
            </p>
            <img src="assets/images/256.png" style="width: 50%; max-width: 800px;">
            <p><strong>Despite having only 20% of the training steps of DiT-XL/2, LiT still competes on par with DiT (2.32 vs. 2.27).</strong>
            </p>
        </div>
        <div class="grid-container-1">
            <!-- <h3 class="grey-heading_nerf_h3">Text-to-video evaluation on VBench.</h3> -->
            <!-- <p><strong>2. Text-to-video evaluation on VBench.</strong> -->
            <img src="assets/images/512.png" style="width: 50%; max-width: 800px;">
            <p><strong> LiT, using pure linear attention, achieves an impressive FID of 3.69, comparable to DiT trained for 3M steps.</strong>
            </p>
        </div>
    </div>


    <!-- Visual Results -->
    <div class="white_section_nerf w-container">
        <h2 class="grey-heading_nerf_h2">Text-to-image Generation</h2>
        <!-- t2i -->
        <div class="grid-container-1">
            <h3 class="grey-heading_nerf_h3">Generated samples</h3>
            <img src="assets/images/t2i_result.png" style="width: 100%; max-width: 800px;">
            <figcaption>
                <p><strong>Generated samples of LiT following user instructions</strong>. LiT shares the same macro/micro-level design as PixArt-Σ,
                    but elegantly replaces all self-attention with cheap linear attention. While being more simple and efficient, LiT with our cost-effective
                    training strategy, is still able to generate exceptional high-resolution images following complicated user instructions.
                </p>
            </figcaption>
        </div>

        <div class="grid-container-1">
            <h2 class="grey-heading_nerf_h2">Offline deployment of on a Windows 11 laptop</h2>
                <div class="grid-container-video-ex">
                    <!-- 第一列 -->
                    <video muted autoplay controls loop class="video-item" style="width: 100%; max-width: 800px;">
                        <source src="assets/videos/demo_long.mp4" type="video/mp4">
                    </video>
                </div>
        </div>

    </div>


    <!-- BibTeX -->
    <div class="white_section_nerf grey_container w-container">
        <h2 class="grey-heading_nerf_h2">BibTeX</h2>
        <div class="bibtex">
            <pre><code>
                @article{wang2025lit,
                title={LiT: Delving into a Simplified Linear Diffusion Transformer for Image Generation},
                author={Wang, Jiahao and Kang, Ning and Yao, Lewei and Chen, Mengzhao and Wu, Chengyue and Zhang, Songyang and Xue, Shuchen and Liu, Yong and Wu, Taiqiang and Liu, Xihui and others},
                journal={arXiv preprint arXiv:2501.12976},
                year={2025}
                }</code></pre>
        </div>
    </div>

</body>
<footer>
    This project page is inspired by <a href="https://bitterdhg.github.io/NOVA_page/">NOVA.</a>.
    All rights reserved.
</footer>

</html>